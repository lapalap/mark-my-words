{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Training the fine-tuned models\n",
    "\n",
    "Code is based on the following Kaggle Notebook:\n",
    "https://www.kaggle.com/code/sovitrath/caltech-256-amp-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install albumentations\n",
    "!pip install pretrainedmodels\n",
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watermark detection\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "transforms = torchvision.transforms.Compose([\n",
    "                           torchvision.transforms.ToTensor(),\n",
    "                           torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                            std=[0.229, 0.224, 0.225])\n",
    "                       ])\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import cv2\n",
    "\n",
    "class_names = ['baseline',\n",
    "               'chinese',\n",
    "               'latin',\n",
    "               'hindi',\n",
    "               'arabic_numerals'\n",
    "              ]\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self,root,transform):\n",
    "        self.root=root\n",
    "        self.transform=transform\n",
    "\n",
    "        self.image_names=glob.glob(self.root + '*.JPEG')\n",
    "        self.image_names.sort()\n",
    "   \n",
    "    #The __len__ function returns the number of samples in our dataset.\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    " \n",
    "    def __getitem__(self,index):\n",
    "        image=cv2.imread(self.image_names[index])\n",
    "        image=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        image=self.transform(image)\n",
    "\n",
    "        return image\n",
    "    \n",
    "    \n",
    "with torch.no_grad():\n",
    "    activation = {}\n",
    "    model_name = 'densenet161'\n",
    "    model = torchvision.models.densenet161(pretrained = True).to(device)\n",
    "    d = 2208\n",
    "    \n",
    "    def get_activation(name):\n",
    "            def hook(model, input, output):\n",
    "                activation[name] = output.mean(axis = [2,3])\n",
    "            return hook\n",
    "    model.features.register_forward_hook(get_activation('features'))\n",
    "    model.eval()\n",
    "\n",
    "    logit_scores = torch.zeros([998, 5, d])\n",
    "\n",
    "    for c, class_name in enumerate(class_names):\n",
    "        dataset = ImageDataset('../dataset/{class_name}/'.format(class_name = class_name),\n",
    "                               transforms\n",
    "                              )\n",
    "        testloader = torch.utils.data.DataLoader(dataset,\n",
    "                                      batch_size=512,\n",
    "                                      shuffle=False,\n",
    "                                      num_workers=2)\n",
    "\n",
    "        counter = 0\n",
    "        with torch.no_grad():\n",
    "            for i, x in tqdm(enumerate(testloader)):\n",
    "                x = x.float().data.to(device)\n",
    "\n",
    "                outputs = model(x)\n",
    "                logit_scores[counter:counter + x.shape[0],c,:] = activation[\"features\"]\n",
    "\n",
    "                counter += x.shape[0]\n",
    "\n",
    "    torch.save(logit_scores, '../activations/{name}_features_wtrmrks.tnsr'.format(name = model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "d = 2208\n",
    "target = torch.tensor([0 if x<998 else 1 for x in range(2*998)])\n",
    "\n",
    "k= 0\n",
    "with torch.no_grad():\n",
    "    acitvations = torch.load('../activations/densenet161_features_wtrmrks.tnsr')\n",
    "    aucs = torch.zeros([4, d])\n",
    "\n",
    "    for j in range(4):\n",
    "        for r in range(d):\n",
    "            aucs[j, r] = torchmetrics.functional.classification.binary_auroc(torch.cat((acitvations[:, 0, r], acitvations[:, j+1, r]), 0), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "%%writefile preprocess.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "# put the path to the dataset\n",
    "root_dir = '../input/caltech256/256_ObjectCategories'\n",
    "# get all the folder paths\n",
    "all_paths = os.listdir(root_dir)\n",
    "\n",
    "# create a DataFrame\n",
    "data = pd.DataFrame()\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "counter = 0\n",
    "for folder_path in tqdm(all_paths, total=len(all_paths)):\n",
    "    # get all the image names in the particular folder\n",
    "    image_paths = os.listdir(f\"{root_dir}/{folder_path}\")\n",
    "    # get the folder as label\n",
    "    label = folder_path.split('.')[-1]\n",
    "    \n",
    "    if label == 'clutter':\n",
    "        continue\n",
    "\n",
    "    # save image paths in the DataFrame\n",
    "    for image_path in image_paths:\n",
    "        if image_path.split('.')[-1] == 'jpg':\n",
    "            data.loc[counter, 'image_path'] = f\"{root_dir}/{folder_path}/{image_path}\"\n",
    "            labels.append(label)\n",
    "            counter += 1\n",
    "\n",
    "labels = np.array(labels)\n",
    "# one-hot encode the labels\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "\n",
    "# add the image labels to the dataframe\n",
    "for i in range(len(labels)):\n",
    "    index = np.argmax(labels[i])\n",
    "    data.loc[i, 'target'] = int(index)\n",
    "    \n",
    "# shuffle the dataset\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(f\"Number of labels or classes: {len(lb.classes_)}\")\n",
    "print(f\"The first one hot encoded labels: {labels[0]}\")\n",
    "print(f\"Mapping the first one hot encoded label to its category: {lb.classes_[0]}\")\n",
    "print(f\"Total instances: {len(data)}\")\n",
    " \n",
    "# save as CSV file\n",
    "data.to_csv('data.csv', index=False)\n",
    " \n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dataset.py\n",
    "\n",
    "import albumentations\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# custom dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, images, labels=None, tfms=None):\n",
    "        self.X = images\n",
    "        self.y = labels\n",
    "\n",
    "        # apply augmentations\n",
    "        if tfms == 0: # if validating\n",
    "            self.aug = albumentations.Compose([\n",
    "                albumentations.Resize(224, 224, always_apply=True),\n",
    "            ])\n",
    "        else: # if training\n",
    "            self.aug = albumentations.Compose([\n",
    "                albumentations.Resize(224, 224, always_apply=True),\n",
    "                albumentations.HorizontalFlip(p=0.5),\n",
    "                albumentations.ShiftScaleRotate(\n",
    "                    shift_limit=0.3,\n",
    "                    scale_limit=0.3,\n",
    "                    rotate_limit=15,\n",
    "                    p=0.5\n",
    "                ),\n",
    "            ])\n",
    "         \n",
    "    def __len__(self):\n",
    "        return (len(self.X))\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        image = Image.open(self.X[i])\n",
    "        image = image.convert('RGB')\n",
    "        image = self.aug(image=np.array(image))['image']\n",
    "        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
    "        label = self.y[i]\n",
    "        return {\n",
    "            'image': torch.tensor(image, dtype=torch.float), \n",
    "            'target': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model.py\n",
    "\n",
    "import pretrainedmodels\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "    \n",
    "class DenseNet161(nn.Module):\n",
    "    def __init__(self, percentile = 0.):\n",
    "        super(DenseNet161, self).__init__()\n",
    "        self.model = torchvision.models.densenet161(pretrained = True)\n",
    "        \n",
    "        aucs = torch.load(\"aucs.tnsr\")\n",
    "        \n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        if percentile == 0.:\n",
    "            self.ommited_indx = None\n",
    "        else:\n",
    "            a = torch.quantile((aucs[0]-0.5).abs(), 1-percentile, dim=0, keepdim=True)\n",
    "            self.ommited_indx = ((aucs[0]-0.5).abs() > a).nonzero()\n",
    "        \n",
    "        self.l0 = nn.Linear(2208, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, _, _, _ = x.shape\n",
    "        x = self.model.features(x)\n",
    "        x = F.adaptive_avg_pool2d(x, 1).reshape(batch, -1)\n",
    "        if self.ommited_indx is not None:\n",
    "            x[:, self.ommited_indx] = 0\n",
    "        l0 = self.l0(x)\n",
    "        return l0\n",
    "\n",
    "# model = DenseNet161()\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile engine.py\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "# training function\n",
    "def fit(model, dataloader, optimizer, criterion, train_data, device, use_amp):\n",
    "    print('Training')\n",
    "    if use_amp == 'yes':\n",
    "        scaler = torch.cuda.amp.GradScaler() \n",
    "\n",
    "    model.train()\n",
    "    train_running_loss = 0.0\n",
    "    train_running_correct = 0\n",
    "    for i, data in tqdm(enumerate(dataloader), total=int(len(train_data)/dataloader.batch_size)):\n",
    "        data, target = data['image'].to(device), data['target'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if use_amp == 'yes':\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, target)\n",
    "        \n",
    "        elif use_amp == 'no':\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            \n",
    "        train_running_loss += loss.item()\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        train_running_correct += (preds == target).sum().item()\n",
    "        \n",
    "        if use_amp == 'yes':\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        \n",
    "        elif use_amp == 'no':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "    train_loss = train_running_loss/len(dataloader.dataset)\n",
    "    train_accuracy = 100. * train_running_correct/len(dataloader.dataset)    \n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "# validation function\n",
    "def validate(model, dataloader, optimizer, criterion, val_data, device, use_amp):\n",
    "    print('Validating')\n",
    "    if use_amp == True:\n",
    "        scaler = torch.cuda.amp.GradScaler() \n",
    "        \n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_running_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(dataloader), total=int(len(val_data)/dataloader.batch_size)):\n",
    "            data, target = data['image'].to(device), data['target'].to(device)\n",
    "            \n",
    "            if use_amp == 'yes':\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(data)\n",
    "                    loss = criterion(outputs, target)\n",
    "        \n",
    "            elif use_amp == 'no':\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, target)\n",
    "            \n",
    "            val_running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            val_running_correct += (preds == target).sum().item()\n",
    "        \n",
    "        val_loss = val_running_loss/len(dataloader.dataset)\n",
    "        val_accuracy = 100. * val_running_correct/len(dataloader.dataset)        \n",
    "        return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from model import DenseNet161\n",
    "from dataset import ImageDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from engine import fit, validate\n",
    "\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# build and parse the argument parser\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-b', '--batch-size', dest='batch_size', type=int, \n",
    "                    help='batch size for the dataset', default=512)\n",
    "parser.add_argument('-p', '--percentile', dest='percentile', type=float, \n",
    "                    help='percentile for ommiting representations', default=0.)\n",
    "parser.add_argument('-a', '--use-amp', dest='use_amp', \n",
    "                    help='to use Automatic Mixed Precision or not',\n",
    "                    default='yes', choices=['yes', 'no'])\n",
    "args = vars(parser.parse_args())\n",
    "\n",
    "# learning parameters\n",
    "batch_size = args['batch_size']\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "\n",
    "percentile = args['percentile']\n",
    "print(f\"Percentile: {percentile}\")\n",
    "\n",
    "epochs = 15\n",
    "lr = 0.0001\n",
    "use_amp = args['use_amp']\n",
    "print(f\"Use AMP: {use_amp}\")\n",
    "\n",
    "# computation device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# get the dataset ready\n",
    "df = pd.read_csv('data.csv')\n",
    "X = df.image_path.values # image paths\n",
    "y = df.target.values # targets\n",
    "(xtrain, xtest, ytrain, ytest) = train_test_split(X, y,\n",
    "\ttest_size=0.10, random_state=42)\n",
    "print(f\"Training instances: {len(xtrain)}\")\n",
    "print(f\"Validation instances: {len(xtest)}\")\n",
    "train_data = ImageDataset(xtrain, ytrain, tfms=1)\n",
    "test_data = ImageDataset(xtest, ytest, tfms=0)\n",
    "# dataloaders\n",
    "train_data_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_data_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# model\n",
    "model = DenseNet161(percentile = percentile)\n",
    "model.to(device)\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# total parameters and trainable parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} trainable parameters.\")\n",
    "\n",
    "train_loss , train_accuracy = [], []\n",
    "val_loss , val_accuracy = [], []\n",
    "if use_amp == 'yes':\n",
    "    print('Tranining and validating with Automatic Mixed Precision')\n",
    "elif use_amp == 'no':\n",
    "    print('Tranining and validating without Automatic Mixed Precision')\n",
    "    \n",
    "start = time.time()\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "    train_epoch_loss, train_epoch_accuracy = fit(model, train_data_loader, \n",
    "                                                 optimizer, criterion, \n",
    "                                                 train_data, device, use_amp)\n",
    "    val_epoch_loss, val_epoch_accuracy = validate(model, valid_data_loader, \n",
    "                                                 optimizer, criterion, \n",
    "                                                 test_data, device, use_amp)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    train_accuracy.append(train_epoch_accuracy)\n",
    "    val_loss.append(val_epoch_loss)\n",
    "    val_accuracy.append(val_epoch_accuracy)\n",
    "    print(f\"Train Loss: {train_epoch_loss:.4f}, Train Acc: {train_epoch_accuracy:.2f}\")\n",
    "    print(f'Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_accuracy:.2f}')\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Took {((end-start)/60):.3f} minutes to train for {epochs} epochs\")\n",
    "    \n",
    "# save model checkpoint\n",
    "torch.save({\n",
    "            'epoch': epochs,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'model': model,\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': criterion,\n",
    "            }, f\"{percentile}_model.pth\")\n",
    "\n",
    "# accuracy plots\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(train_accuracy, color='green', label='train accuracy')\n",
    "plt.plot(val_accuracy, color='blue', label='validataion accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.savefig(f\"amp_{use_amp}_accuracy.png\")\n",
    "plt.show()\n",
    " \n",
    "# loss plots\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(train_loss, color='orange', label='train loss')\n",
    "plt.plot(val_loss, color='red', label='validataion loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(f\"amp_{use_amp}_loss.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for percentile in [0, 0.005, 0.01, 0.02, 0.03, 0.05, 0.1, 0.15, 0.25, 0.5]:\n",
    "    torch.cuda.empty_cache()\n",
    "    os.system('python train.py --batch-size 256 --percentile {percentile} --use-amp no'.format(percentile = percentile))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
